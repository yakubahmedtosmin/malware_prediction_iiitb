#!/usr/bin/env python
# coding: utf-8

# ## import libraries

# In[ ]:


import pandas as pd
import numpy as np
import random

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px


# #### Loading dataset

# In[15]:


train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")


# In[16]:


train.head()


# In[17]:


test.head()


# In[18]:


train.shape, test.shape


# ## Exploratory Data Analysis

# #### Visualising the percentage of target values

# In[19]:


Z = train['HasDetections']
train_1 = train.drop(columns=['HasDetections','MachineIdentifier'])
test_1 = test.drop(columns=['MachineIdentifier'])


# In[21]:


train_1.shape, test_1.shape


# In[24]:


fig=px.pie(names=Z.value_counts().index,values=Z.value_counts(),title='Target values')
fig.update_traces(textposition='inside', textinfo='percent+label')
fig.show()


# #### from the above pie plot we can infer that it is the percentange of no detectin is very high and detection percentage is very less. So we can infer that it is an imbalanced dataset

# #### 1. EDA on categorical data

# ##### EDA ON TRAIN DATA

# * Countplots of different categorical data

# In[25]:


plt.figure(figsize=(20,10))
plt.subplot(2,2,1)
sns.countplot(train['ProductName']);
plt.title('Product Name')
plt.xlabel('Products')
plt.subplot(2,2,2)
train_1['Platform'].value_counts().plot.pie(autopct="%.1f%%",colors = ['skyblue', 'purple', 'red','yellow'],
                                                           pctdistance=1.2,labeldistance=1.5,startangle=40);
#plt.title('Platform')
plt.subplot(2,2,3)
train_1['Processor'].value_counts().plot.pie(autopct="%.1f%%",pctdistance=1.2,labeldistance=1.5);
plt.title('Different Processors used')
plt.subplot(2,2,4)
sns.countplot(train['OsVer']);
plt.title('Different Versions of OS')
plt.xticks(rotation=45);


# * Plots with respect to the Census OS

# In[28]:


plt.figure(figsize=(20,20))
plt.subplot(2,3,1)
plt.title('Number of Census_OSBranch with respect to target')
sns.countplot(y ='Census_OSBranch', hue = Z, data = train_1);
plt.subplot(2,3,2)
plt.title('Number of Census_OSArchitecture with respect to target')
sns.countplot(hue= Z, x ='Census_OSArchitecture', data = train_1)
plt.subplot(2,3,3)
plt.title('Number of Census_OSAutoUpdateOptionsName with respect to target')
sns.countplot(x='Census_OSWUAutoUpdateOptionsName',hue=Z,data=train_1);
plt.xticks(rotation=90);
plt.subplot(2,3,4)
plt.title('Number of Census_OSInstallTypeName with respect to target')
sns.countplot(y='Census_OSInstallTypeName',hue=Z,data=train_1);
plt.subplot(2,3,5)
plt.title('Number of Census_OSSkuName with respect to target')
sns.countplot(x='Census_OSSkuName',hue=Z,data=train_1,palette=["#fc9272","#fee0d2"]);
plt.xticks(rotation=90);


# #### from the above plots we can see that in all the features one value is very high w.r.t other .

# In[29]:


plt.figure(figsize=(12,8))
plt.subplot(1,2,1)
train_1['Census_OSVersion'].value_counts().sort_values(ascending=False)[:10].plot.bar(color='red');
plt.subplot(1,2,2)
train_1['Census_OSEdition'].value_counts().sort_values(ascending=False).plot.bar(color='skyblue');


# In[31]:


sns.countplot(x='Census_GenuineStateName',hue=Z,data=train_1);


# In[32]:


train_num=train_1.select_dtypes(exclude='object')
train_num[train_num.columns[train_num.nunique()>10]].hist(bins=50,figsize=(20, 24));


# In[33]:


miss=((train.isnull().sum()/len(train))*100).sort_values(ascending=False)
missdf=pd.DataFrame({'column_name':miss.index, 'Missing_values':miss.values})
m=missdf[missdf['Missing_values']>0]
m=m['column_name'].to_list()
miss_train=train[m]
for i in miss_train:
    print('_______________________')
    print(miss_train[i].isnull().sum())
    print(miss_train[i].value_counts())


# In[34]:


fig = px.bar(missdf.head(43),x='Missing_values',y='column_name',
             orientation='h',height=1500,width=900,color='Missing_values',text='Missing_values',title='Missing value percentage in train dataset')
fig.update_traces(textposition='outside')
fig.show()


# #### missing value imputation

# * train and test dataset on the basis of value counts

# In[9]:


train['Census_ChassisTypeName'].fillna('Notebook',inplace=True)
test['Census_ChassisTypeName'].fillna('Notebook',inplace=True)
train['Census_PrimaryDiskTypeName'].fillna('HDD',inplace=True)
test['Census_PrimaryDiskTypeName'].fillna('HDD',inplace=True)
train['UacLuaenable'].fillna(1.0,inplace=True)
test['UacLuaenable'].fillna(1.0,inplace=True)
train['GeoNameIdentifier'].fillna(277.0,inplace=True)
test['GeoNameIdentifier'].fillna(277.0,inplace=True)
train['Census_IsVirtualDevice'].fillna(0.0,inplace=True)
test['Census_IsVirtualDevice'].fillna(0.0,inplace=True)
train['RtpStateBitfield'].fillna(7.0,inplace=True)
test['RtpStateBitfield'].fillna(7.0,inplace=True)
train['IsProtected'].fillna(1.0,inplace=True)
test['IsProtected'].fillna(1.0,inplace=True)
train['Census_ProcessorCoreCount'].fillna(4.0,inplace=True)
test['Census_ProcessorCoreCount'].fillna(4.0,inplace=True)
train['Census_ProcessorManufacturerIdentifier'].fillna(5.0,inplace=True)
test['Census_ProcessorManufacturerIdentifier'].fillna(5.0,inplace=True)
train['IsProtected'].fillna(1.0,inplace=True)
test['IsProtected'].fillna(1.0,inplace=True)
train['AVProductStatesIdentifier'].fillna(53447.0,inplace=True)
test['AVProductStatesIdentifier'].fillna(53447.0,inplace=True)
train['AVProductsEnabled'].fillna(1.0,inplace=True)
test['AVProductsEnabled'].fillna(1.0,inplace=True)
train['AVProductsInstalled'].fillna(1.0,inplace=True)
test['AVProductsInstalled'].fillna(1.0,inplace=True)
train['Census_InternalPrimaryDisplayResolutionHorizontal'].fillna(1366.0,inplace=True)
test['Census_InternalPrimaryDisplayResolutionHorizontal'].fillna(1366.0,inplace=True)
train['Census_InternalPrimaryDisplayResolutionVertical'].fillna(768.0,inplace=True)
test['Census_InternalPrimaryDisplayResolutionVertical'].fillna(768.0,inplace=True)
train['Census_InternalPrimaryDiagonalDisplaySizeInInches'].fillna(15.5,inplace=True)
test['Census_InternalPrimaryDiagonalDisplaySizeInInches'].fillna(15.5,inplace=True)
train['Census_OSInstallLanguageIdentifier'].fillna(8.0,inplace=True)
test['Census_OSInstallLanguageIdentifier'].fillna(8.0,inplace=True)
train['IeVerIdentifier'].fillna(137.0,inplace=True)
test['IeVerIdentifier'].fillna(137.0,inplace=True)
train['Census_IsAlwaysOnAlwaysConnectedCapable'].fillna(0.0,inplace=True)
test['Census_IsAlwaysOnAlwaysConnectedCapable'].fillna(0.0,inplace=True)
train['Census_TotalPhysicalRAM'].fillna(4096.0,inplace=True)
test['Census_TotalPhysicalRAM'].fillna(4096.0,inplace=True)
train['Firewall'].fillna(0,inplace=True)
test['Firewall'].fillna(0,inplace=True)
train['Census_IsFlightsDisabled'].fillna(0,inplace=True)
test['Census_IsFlightsDisabled'].fillna(0,inplace=True)
train['Census_InternalBatteryNumberOfCharges'].fillna(0,inplace=True)
test['Census_InternalBatteryNumberOfCharges'].fillna(0,inplace=True)
train['Wdft_IsGamer'].fillna(0,inplace=True)
test['Wdft_IsGamer'].fillna(0,inplace=True)
train['SMode'].fillna(0,inplace=True)
test['SMode'].fillna(0,inplace=True)
train['OrganizationIdentifier'].fillna(27.0,inplace=True)
test['OrganizationIdentifier'].fillna(27.0,inplace=True)


# In[27]:


test['CityIdentifier'].fillna(130775.0, inplace = True)
test['OsBuildLab'].fillna('17134.1.amd64fre.rs4_release.180410-1804 ', inplace=True)
test['Census_OEMNameIdentifier'].fillna(2668.0, inplace = True)
test['Census_OEMModelIdentifier'].fillna(313586.0, inplace = True)
test['Census_ProcessorModelIdentifier'].fillna(1998.0, inplace = True)
test['Census_PrimaryDiskTotalCapacity'].fillna(476940.0, inplace = True)
test['Census_SystemVolumeTotalCapacity'].fillna(28542.0, inplace = True)
test['Census_PowerPlatformRoleName'].fillna('Mobile', inplace = True)
test['Census_FirmwareManufacturerIdentifier'].fillna(142.0, inplace = True)
test['Census_FirmwareVersionIdentifier'].fillna(33105.0, inplace = True)
test['Wdft_RegionIdentifier'].fillna(10.0)


# In[11]:


lis = train.columns[train.isnull().sum()>0]
index = 0
for i in lis:
    print("-------------------------------------------------------------")
    print(index)
    index+=1
    print(i)
    print(train[i].isnull().sum()/len(train))           ## normalise the null value counts inevery columns
    print(train[i].value_counts())                      ## value_counts 
    x= train[i].value_counts().reset_index().iloc[0][1] ## index reseting w.r.t value counts for importance
    print(x/len(train))                                 ## print the normalise value 


# In[12]:


import gc
for i in [0,2,3,7,10,11,12,15]:                     ## removing the columns from the index getting from previous code 
    train.drop(columns = lis[i],inplace = True)     
    test.drop(columns = lis[i],inplace = True)
gc.collect()                                         


# In[13]:


for i in [1,4,5,6,8,9,13,14,16]:                   ## fill the null values and reset the index from the index we get from the previous code
    train[lis[i]].fillna(train[lis[i]].value_counts().reset_index().iloc[0][0],inplace=True)


# #### Encoding the categorical values

# In[14]:


from sklearn.preprocessing import LabelEncoder   
le = LabelEncoder()


# In[15]:


train.drop(columns=['MachineIdentifier'],inplace=True)   ## before encodeing drop the target column


# In[16]:


for col in train.columns:                                ## loop to encode the categoricla data and count for proof in the training set
    print(col)
    if train[col].dtype == 'object':
        le.fit(train[col])
        train[col] = le.transform(train[col])


# In[18]:


for i in [1,4,5,6,8,9,13,14,16]:               ## fill the null values and reset the index from the index we get from the previous code
    test[lis[i]].fillna(test[lis[i]].value_counts().reset_index().iloc[0][0],inplace=True)


# In[28]:


print(train.columns[train.isnull().sum()>0])     ## verify that all the null values are imputed
print(test.columns[test.isnull().sum()>0])  


# In[29]:


for col in test.columns:                        ## loop to encode the categoricla data and count for proof in the test set
    print(col)
    if test[col].dtype == 'object':
        le.fit(test[col])
        test[col] = le.transform(test[col])


# In[31]:


#!pip install xgboost


# ## Model Building

# #### ADABOOST

# In[41]:


from sklearn.ensemble import AdaBoostClassifier     ## Import the mnodel library


# In[42]:


AdaBoostClassifier().get_params()                   ## getting the hyperparamerets in the model


# In[43]:


clf_ada = AdaBoostClassifier(algorithm='SAMME',      ## tweaking the default parameters with small steps   
                             base_estimator=None,
                             learning_rate= 1.5,
                             n_estimators=55,
                             random_state= None) 
# n_estimator: The maximum number of estimators at which boosting is terminated
# learning_ratefloat, default=1.
  #Learning rate shrinks the contribution of each classifier by learning_rate. 
#algorithm{‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’
  #If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. 
  #base_estimator must support calculation of class probabilities. 
  #If ‘SAMME’ then we should use SAMME discrete boosting algorithm.


# #### feature selection algorithm

# In[44]:


from sklearn.feature_selection import RFECV                      ## using recursion feature elimination
from sklearn.model_selection import KFold, StratifiedKFold       ## cross validation library as not test train split is done 
rfe = RFECV(estimator=clf_ada, step=10, cv=KFold(n_splits=5, shuffle=False),
            scoring='roc_auc', verbose=2)                        ## fit the model in the recursion feature elimination


# #### making the test train data set

# In[37]:


X = train.iloc[:,:73 ]
Y = train.iloc[:, -1:]


# #### cross validation and calculate the train and test score.

# In[46]:


cv = KFold(n_splits=5, random_state = None, shuffle=True)
scores = []
for (train1, test1), i in zip(cv.split(X, Y), range(5)):
    rfe.fit(X.iloc[train1], Y.iloc[train1])
    train_score = rfe.score(X.iloc[train1], Y.iloc[train1])
    test_score = rfe.score(X.iloc[test1], Y.iloc[test1])
    scores.append((train_score, test_score))
    
pd.DataFrame(scores, columns=['Train', 'Test'])


# In[47]:


print('Optimal number of features:', rfe.n_features_)   ## printing the optimal feature after RFE


# #### plotting the AUC ROC graph to see the model score

# In[48]:


import matplotlib.pyplot as plt
plt.figure(figsize=(14, 8))
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score")
plt.plot(range(1, len(rfe.grid_scores_) + 1), rfe.grid_scores_)
plt.show()


# #### put the top optimal feature in a list to fit the model

# In[49]:


lst= []
for col in X.columns[rfe.ranking_ == 1]:
    #print(col)
    lst.append(col)


# In[2]:


lst


# #### prepare the training and test data columns with optimal feature

# In[51]:


X_1 = X[['AppVersion',
 'AvSigVersion',
 'IsBeta',
 'RtpStateBitfield',
 'IsSxsPassiveMode',
 'AVProductStatesIdentifier',
 'AVProductsInstalled',
 'HasTpm',
 'LocaleEnglishNameIdentifier',
 'Processor',
 'Census_PrimaryDiskTotalCapacity',
 'Census_SystemVolumeTotalCapacity',
 'Census_TotalPhysicalRAM',
 'Census_InternalPrimaryDiagonalDisplaySizeInInches',
 'Census_OSInstallTypeName',
 'Census_OSInstallLanguageIdentifier',
 'Census_ActivationChannel',
 'Census_IsVirtualDevice',
 'Census_IsTouchEnabled',
 'Census_IsPenCapable',
 'Census_IsAlwaysOnAlwaysConnectedCapable',
 'Wdft_IsGamer',
 'Wdft_RegionIdentifier']]


# In[52]:


test3 = test[['AppVersion',
 'AvSigVersion',
 'IsBeta',
 'RtpStateBitfield',
 'IsSxsPassiveMode',
 'AVProductStatesIdentifier',
 'AVProductsInstalled',
 'HasTpm',
 'LocaleEnglishNameIdentifier',
 'Processor',
 'Census_PrimaryDiskTotalCapacity',
 'Census_SystemVolumeTotalCapacity',
 'Census_TotalPhysicalRAM',
 'Census_InternalPrimaryDiagonalDisplaySizeInInches',
 'Census_OSInstallTypeName',
 'Census_OSInstallLanguageIdentifier',
 'Census_ActivationChannel',
 'Census_IsVirtualDevice',
 'Census_IsTouchEnabled',
 'Census_IsPenCapable',
 'Census_IsAlwaysOnAlwaysConnectedCapable',
 'Wdft_IsGamer',
 'Wdft_RegionIdentifier']]


# #### fitting the model with training data 

# In[53]:


clf_ada.fit(X_1, Y)


# #### predict the probability values of the HasDetection Column

# In[55]:


a= np.array(clf_ada.predict_proba(test3))[:,1].T


# In[62]:


a


# In[65]:


t=pd.read_csv("test.csv")


# #### building the submitted file

# In[57]:


predictions=pd.DataFrame()


# In[66]:


predictions['MachineIdentifier']=t['MachineIdentifier']
predictions['HasDetections']=a


# In[61]:


predictions['MachineIdentifier']


# In[69]:


predictions.to_csv('submitted11.csv',index=False)


# In[70]:


predictions


# In[ ]:




